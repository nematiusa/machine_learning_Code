{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7- A Comparison of Shrinkage and Selection Methods for Linear Regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCpfuV1zMcYB+1rZzUVKYg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iXUw-kYVVufr","colab_type":"text"},"source":["# Model Selection\n","\n","When you have many predictor variables in a predictive model, the model selection methods allow to select automatically the best combination of predictor variables for building an optimal predictive model.\n","\n","Removing irrelevant variables leads a more interpretable and a simpler model. With the same performance, a simpler model should be always used in preference to a more complex model.\n","\n","Additionally, the use of model selection approaches is critical in some situations, where you have a large multivariate data sets with many predictor variables. This is often the case in genomic area, where a substantial challenge comes from the fact that the number of genomic variables (p) is usually much larger than the number of individuals (n) (i.e., p >> n) (Bovelstad et al. 2007).\n","\n","It’s well known that, when p >> n, it is easy to find predictors that perform excellently on the fitted data, but fail in external validation, leading to poor prediction rules. Furthermore, there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training (James et al. 2014).\n","\n","One possible strategy consists of testing all possible combination of the predictors, and then selecting the best model. This method called best subsets regression  is computationally expensive and becomes unfeasible for a large data set with many variables.\n","\n","A better alternative to the best subsets regression is to use the stepwise regression (Chapter @ref(stepwise-regression)) method, which consists of adding and deleting predictors in order to find the best performing model with a reduced set of variables .\n","\n","Other methods for high-dimensional data, containing multiple predictor variables, include the penalized regression (ridge and lasso regression, and the principal components-based regression methods ."]},{"cell_type":"markdown","metadata":{"id":"zJUZEVWsWEaD","colab_type":"text"},"source":["The stepwise regression (or stepwise selection) consists of iteratively adding and removing predictors, in the predictive model, in order to find the subset of variables in the data set resulting in the best performing model, that is a model that lowers prediction error.\n","\n","There are three strategies of stepwise regression (James et al. 2014,P. Bruce and Bruce (2017)):\n","\n","1. Forward selection, which starts with no predictors in the model, iteratively adds the most contributive predictors, and stops when the improvement is no longer statistically significant.\n","2. Backward selection (or backward elimination), which starts with all predictors in the model (full model), iteratively removes the least contributive predictors, and stops when you have a model where all predictors are statistically significant.\n","3. Stepwise selection (or sequential replacement), which is a combination of forward and backward selections. You start with no predictors, then sequentially add the most contributive predictors (like forward selection). After adding each new variable, remove any variables that no longer provide an improvement in the model fit (like backward selection)."]},{"cell_type":"markdown","metadata":{"id":"ZaMtIBmAWWjv","colab_type":"text"},"source":["# Penalized Regression Essentials: Ridge, Lasso & Elastic Net\n","\n","The standard linear model (or the ordinary least squares method) performs poorly in a situation, where you have a large multivariate data set containing a number of variables superior to the number of samples.\n","\n","A better alternative is the penalized regression allowing to create a linear regression model that is penalized, for having too many variables in the model, by adding a constraint in the equation (James et al. 2014,P. Bruce and Bruce (2017)). This is also known as shrinkage or regularization methods.\n","\n","The consequence of imposing this penalty, is to reduce (i.e. shrink) the coefficient values towards zero. This allows the less contributive variables to have a coefficient close to zero or equal zero.\n","\n","Note that, the shrinkage requires the selection of a tuning parameter (lambda) that determines the amount of shrinkage."]},{"cell_type":"markdown","metadata":{"id":"nHmkX7yXXAdE","colab_type":"text"},"source":["# Why shrink or subset and what does this mean?\n","In the linear regression context, subsetting means choosing a subset from available variables to include in the model, thus reducing its dimensionality. Shrinkage, on the other hand, means reducing the size of the coefficient estimates (shrinking them towards zero). Note that if a coefficient gets shrunk to exactly zero, the corresponding variable drops out of the model. Consequently, such a case can also be seen as a kind of subsetting.\n","\n","Shrinkage and selection aim at improving upon the simple linear regression. There are two main reasons why it could need an improvement:\n","\n","* Prediction accuracy: Linear regression estimates tend to have low bias and high variance. Reducing model complexity (the number of parameters that need to be estimated) results in reducing the variance at the cost of introducing more bias. If we could find the sweet spot where the total error, so the error resulting from bias plus the one from variance, is minizmized, we can improve the model’s predictions.\n","\n","* Model’s interpretability: With too many predictors it is hard for a human to grasp all the relations between the variables. In some cases we would be willing to determine a small subset of variables with the strongest impact, thus sacrificing some details in order to get the big picture."]},{"cell_type":"markdown","metadata":{"id":"1YxMgF6aXdiV","colab_type":"text"},"source":["# Setup & Data Load"]},{"cell_type":"code","metadata":{"id":"pLybUIrzXhlm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1592449298779,"user_tz":300,"elapsed":437,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"254c1b5c-3485-47e0-dcfd-9a1d10a00747"},"source":["# Import necessary modules and set options\n","import pandas as pd\n","import numpy as np\n","import itertools\n","\n","from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV\n","from sklearn.cross_decomposition import PLSRegression\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load data\n","data = pd.read_csv(\"prostate.data\", sep = \"\\t\")\n","print(data.head())\n","\n","# Train-test split\n","y_train = np.array(data[data.train == \"T\"]['lpsa'])\n","y_test = np.array(data[data.train == \"F\"]['lpsa'])\n","X_train = np.array(data[data.train == \"T\"].drop(['lpsa', 'train'], axis=1))\n","X_test = np.array(data[data.train == \"F\"].drop(['lpsa', 'train'], axis=1))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["   Unnamed: 0    lcavol   lweight  age  ...  gleason  pgg45      lpsa  train\n","0           1 -0.579818  2.769459   50  ...        6      0 -0.430783      T\n","1           2 -0.994252  3.319626   58  ...        6      0 -0.162519      T\n","2           3 -0.510826  2.691243   74  ...        7     20 -0.162519      T\n","3           4 -1.203973  3.282789   58  ...        6      0 -0.162519      T\n","4           5  0.751416  3.432373   62  ...        6      0  0.371564      T\n","\n","[5 rows x 11 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PLAPSJHfYjwO","colab_type":"text"},"source":["# Linear Regression\n","\n","Let us start with the simple linear regression, which will constitute our benchmark. It models the target variable, y, as a linear combination of p predictors, or features, X:"]},{"cell_type":"code","metadata":{"id":"9cQSyDufYd8u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":115},"executionInfo":{"status":"ok","timestamp":1592449435006,"user_tz":300,"elapsed":361,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"7a467c13-9e40-419c-9b16-db2da908276a"},"source":["from IPython.display import Image\n","from IPython.core.display import HTML \n","Image(url= \"https://miro.medium.com/max/564/1*1T261QLMnn9ApzSJZ_6L-w.png\")"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/564/1*1T261QLMnn9ApzSJZ_6L-w.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"9EaTEzTpYmsM","colab_type":"text"},"source":["This model has p + 2 parameters that have to be estimated from the training data:\n","\n","* The p feature β-coefficients, one per variable, denoting their impacts on the target;\n","* One intercept parameter, denoted as β0 above, which is the prediction in case all Xs are zero. It is not necessary to include it in the model, and indeed in some cases it should be dropped (e.g. if one wants to include a full set of dummies denoting levels of a categorical variable) but in general it gives the model more flexibility, as you will see in the next paragraph;\n","* One variance parameter of the Gaussian error term.\n","\n","These parameters are typically estimated using the Ordinary Least Square (OLS). OLS minimizes the sum of squared residuals, given by"]},{"cell_type":"code","metadata":{"id":"Z0AKTawhYr8J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"status":"ok","timestamp":1592449492436,"user_tz":300,"elapsed":349,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"bfc0ac9e-23df-4f13-f50b-037b5ae4465d"},"source":["from IPython.display import Image\n","from IPython.core.display import HTML \n","Image(url= \"https://miro.medium.com/max/964/1*-FDUVzZCw-5OvMOH2Je3hQ.png\")"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/964/1*-FDUVzZCw-5OvMOH2Je3hQ.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"jL4MdHNkZpih","colab_type":"text"},"source":["It is helpful to think about this minimization criterion graphically. With only one predictor X, we are in a 2D space, formed by this predictor and the target. In this setting, the model fits such a line in the X-Y space that is the closest to all data points, with the proximity measured as the sum of squared vertical distances of all data points — see the left panel below. If there are two predictors, X1 and X2, the space grows to 3D and now the model fits a plane that is closest to all points in the 3D space — see the right panel below. With more than two features, the plane becomes the somewhat abstract hyperplane, but the idea is still the same. These visualisations also help to see how the intercept gives the model more flexibility: if it is included, it allows the line or plane to not cross the space’s origin.\n"]},{"cell_type":"code","metadata":{"id":"jCXSIZEYZrjy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"ok","timestamp":1592449780740,"user_tz":300,"elapsed":445,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"991dffba-addd-472f-cdb6-950798f45a5c"},"source":["Image(url= \"https://miro.medium.com/max/1400/1*CSrJ-XXGy9iVMaCRDOPFQg.png\")"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/1400/1*CSrJ-XXGy9iVMaCRDOPFQg.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"pQkDZEFwaGhP","colab_type":"text"},"source":["The minimization problem described above turns out to have an analytical solution, and the β-parameters can be calculated as"]},{"cell_type":"code","metadata":{"id":"rkMtDFa_Z0R9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"executionInfo":{"status":"ok","timestamp":1592449782930,"user_tz":300,"elapsed":437,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"d1b4aace-8c87-4444-c12f-4943dd153ff1"},"source":["Image(url= \"https://miro.medium.com/max/191/1*SOihShh-7bIvAlsjzL4nUQ.png\")"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/191/1*SOihShh-7bIvAlsjzL4nUQ.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"c5zr3oDYaC6S","colab_type":"text"},"source":["Including a column of ones in the X matrix allows to express the intercept part of the β-hat vector in the formula above. The “hat” above the “β” denotes that it is an estimated value, based on the training data."]},{"cell_type":"markdown","metadata":{"id":"ecULgqUWaORK","colab_type":"text"},"source":["# The Bias-Variance trade-off\n","\n","In statistics, there are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator. It measures the inaccuracy of the estimates. The variance, on the other hand, measures the spread between them."]},{"cell_type":"code","metadata":{"id":"Jm46bV_VaUwO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":463},"executionInfo":{"status":"ok","timestamp":1592449921523,"user_tz":300,"elapsed":360,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"f184a947-817e-4a7a-a1a5-ad728fe57f28"},"source":["Image(url= \"https://miro.medium.com/max/896/1*jngqFdNagZhMMoSS5UyHoQ.jpeg\")"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/896/1*jngqFdNagZhMMoSS5UyHoQ.jpeg\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"JvwYgWQuacTm","colab_type":"text"},"source":["Clearly, both bias and variance can harm the model’s predictive performance if they are too large. The linear regression, however, tends to suffer from variance, while having a low bias. This is especially the case if there are many predictive features in the model or if they are highly correlated with each other. This is where subsetting and regularization come to rescue. They allow to reduce the variance at the cost of introducing some bias, ultimately reducing the total error of the model.\n","\n","Before discussing these methods in detail, let us fit linear regression to out prostate data and check its out-of-sample Mean Prediction Error (MAE)."]},{"cell_type":"code","metadata":{"id":"Im1llos-X7Ds","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1592449341875,"user_tz":300,"elapsed":328,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"603de42f-f34e-495e-e45d-53c8b5e21048"},"source":["linreg_model = LinearRegression(normalize=True).fit(X_train, y_train)\n","linreg_prediction = linreg_model.predict(X_test)\n","linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n","linreg_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((linreg_model.intercept_, linreg_model.coef_), \n","        axis=None), 3))\n",")\n","\n","print('Linear Regression MAE: {}'.format(np.round(linreg_mae, 3)))\n","print('Linear Regression coefficients:')\n","linreg_coefs"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Linear Regression MAE: 0.24\n","Linear Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 1.014,\n"," 'Unnamed: 0': 0.034,\n"," 'age': -0.007,\n"," 'gleason': -0.042,\n"," 'lbph': 0.046,\n"," 'lcavol': 0.145,\n"," 'lcp': -0.052,\n"," 'lweight': 0.056,\n"," 'pgg45': 0.003,\n"," 'svi': 0.066}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"1SYlVCmmaf5R","colab_type":"text"},"source":["# Best Subset Regression\n","\n","A straightforward approach to choosing a subset of variables for linear regression is to try all possible combinations and pick one that minimizes some criterion. This is what Best Subset Regression aims for. For every k ∈ {1, 2, …, p}, where p is the total number of available features, it picks the subset of size k that gives the smallest residual sum of squares. However, sum of squares cannot be used as a criterion to determine k itself, as it is necessarily decreasing with k: the more variables are included in the model, the smaller are its residuals. This does not guarantee better predictive performance though. That’s why another criterion should be used to select the final model. For models focused on prediction, a (possibly cross-validated) error on test data is a common choice.\n","\n","As Best Subset Regression is not implemented in any Python package, we have to loop over k and all subsets of size k manually. The following chunk of code does the job."]},{"cell_type":"code","metadata":{"id":"13oCAkqmbGYp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1592450118685,"user_tz":300,"elapsed":1694,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"6c1293be-eecf-4626-f949-dd3140f63151"},"source":["results = pd.DataFrame(columns=['num_features', 'features', 'MAE'])\n","\n","# Loop over all possible numbers of features to be included\n","for k in range(1, X_train.shape[1] + 1):\n","    # Loop over all possible subsets of size k\n","    for subset in itertools.combinations(range(X_train.shape[1]), k):\n","        subset = list(subset)\n","        linreg_model = LinearRegression(normalize=True).fit(X_train[:, subset], y_train)\n","        linreg_prediction = linreg_model.predict(X_test[:, subset])\n","        linreg_mae = np.mean(np.abs(y_test - linreg_prediction))\n","        results = results.append(pd.DataFrame([{'num_features': k,\n","                                                'features': subset,\n","                                                'MAE': linreg_mae}]))\n","\n","# Inspect best combinations\n","results = results.sort_values('MAE').reset_index()\n","print(results.head())\n","\n","# Fit best model\n","best_subset_model = LinearRegression(normalize=True).fit(X_train[:, results['features'][0]], y_train)\n","best_subset_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((best_subset_model.intercept_, best_subset_model.coef_), axis=None), 3))\n",")\n","\n","print('Best Subset Regression MAE: {}'.format(np.round(results['MAE'][0], 3)))\n","print('Best Subset Regression coefficients:')\n","best_subset_coefs"],"execution_count":13,"outputs":[{"output_type":"stream","text":["   index num_features            features       MAE\n","0      0            4        [0, 2, 3, 4]  0.221212\n","1      0            3           [0, 3, 4]  0.222467\n","2      0            3           [0, 2, 3]  0.222898\n","3      0            6  [0, 1, 2, 3, 4, 6]  0.224248\n","4      0            5     [0, 1, 3, 4, 6]  0.224347\n","Best Subset Regression MAE: 0.221\n","Best Subset Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 0.607,\n"," 'Unnamed: 0': 0.039,\n"," 'age': 0.027,\n"," 'lcavol': 0.025,\n"," 'lweight': -0.002}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"ylgStpFudC4u","colab_type":"text"},"source":["# Ridge Regression\n","\n","One drawback of Best Subset Regression is that it does not tell us anything about the impact of the variables that are excluded from the model on the response variable. Ridge Regression provides an alternative to this hard selection of variables that splits them into incldued in and excluded from the model. Instead, it penalizes the coefficients to shrink them towards zero. Not exactly zero, as that would mean exlusion from the model, but in the direction of zero, which can be viewed as decreasing model’s complexity in a continuous way, while keeping all variables in the model.\n","\n","In Ridge Regression, the Linear Regression loss function is augmented in such a way to not only minimize the sum of squared residuals but also to penalize the size of parameter estimates:"]},{"cell_type":"code","metadata":{"id":"uCksstbydHDG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1592450651042,"user_tz":300,"elapsed":364,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"971284d0-f011-468e-9062-562ee3c2825b"},"source":["Image(url= \"https://miro.medium.com/max/974/1*wJE2pjOmTATRF78cHabpwQ.png\")"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/974/1*wJE2pjOmTATRF78cHabpwQ.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"1EbJtq2ndJhH","colab_type":"text"},"source":["Solving this minimization problem results in an analytical formula for the βs:"]},{"cell_type":"code","metadata":{"id":"o1G-WnJ1dLfF","colab_type":"code","colab":{}},"source":["Image(url= \"https://miro.medium.com/max/264/1*dLXDbWeBRkCpccHD-LrRUA.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7ilb7NrdPHr","colab_type":"text"},"source":["where I denotes an identity matrix. The penalty term λ is a hyperparameter to be chosen: the larger its value, the more are the coefficients shrinked towards zero. One can see from the formula above that as λ goes to zero, the additive penalty vanishes and β-ridge becomes the same as β-OLS from linear regression. On the other hand, as λ grows to infinity, β-ridge approaches zero: with high enough penalty, coefficients can be shrinked arbitrarily close to zero.\n","\n","But does this shrinkage really result in reducing the variance of the model at the cost of introducing some bias as promised? Yes, it does, which is clear from the formulae for ridge regression estimates’ bias and variance: as λ increases, so does the bias, while the variance goes down!"]},{"cell_type":"code","metadata":{"id":"loWNsxgqdTGV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592450701008,"user_tz":300,"elapsed":325,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"2efff99d-9117-466b-feea-1f5c19540d98"},"source":["Image(url=\"https://miro.medium.com/max/810/1*_1qpxhlhtLcpfO7dQ_bS9g.png\")"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/810/1*_1qpxhlhtLcpfO7dQ_bS9g.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"oOGe4zN-dXrz","colab_type":"text"},"source":["Now, how to choose to best value for λ? Run cross-validation trying a set of different values and pick one that minimizes cross-validated error on test data. Luckily, Python’s scikit-learn can do this for us."]},{"cell_type":"code","metadata":{"id":"5BoPHe4AdZ7S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1592450721974,"user_tz":300,"elapsed":377,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"90c0bd6a-ef40-4f02-d6ef-d17938dfcd0f"},"source":["ridge_cv = RidgeCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n","ridge_model = ridge_cv.fit(X_train, y_train)\n","ridge_prediction = ridge_model.predict(X_test)\n","ridge_mae = np.mean(np.abs(y_test - ridge_prediction))\n","ridge_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((ridge_model.intercept_, ridge_model.coef_), \n","                                axis=None), 3))\n",")\n","\n","print('Ridge Regression MAE: {}'.format(np.round(ridge_mae, 3)))\n","print('Ridge Regression coefficients:')\n","ridge_coefs"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Ridge Regression MAE: 0.239\n","Ridge Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 0.948,\n"," 'Unnamed: 0': 0.033,\n"," 'age': -0.007,\n"," 'gleason': -0.035,\n"," 'lbph': 0.049,\n"," 'lcavol': 0.154,\n"," 'lcp': -0.051,\n"," 'lweight': 0.076,\n"," 'pgg45': 0.003,\n"," 'svi': 0.088}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"h4F2aoHmdlFB","colab_type":"text"},"source":["# LASSO -> L1 Regularization\n","\n","Lasso, or Least Absolute Shrinkage and Selection Operator, is very similar in spirit to Ridge Regression. It also adds a penalty for non-zero coefficients to the loss function, but unlike Ridge Regression which penalizes sum of squared coefficients (the so-called L2 penalty), LASSO penalizes the sum of their absolute values (L1 penalty). As a result, for high values of λ, many coefficients are exactly zeroed under LASSO, which is never the case in Ridge Regression.\n","\n","Another important difference between them is how they tackle the issue of multicollinearity between the features. In Ridge Regression, the coefficients of correlated variables tend be similar, while in LASSO one of them is usually zeroed and the other is assigned the entire impact. Because of this, Ridge Regression is expected to work better if there are many large parameters of about the same value, i.e. when most predictors truly impact the response. LASSO, on the other hand, is expected to come on top when there are a small number of significant parameters and the others are close to zero, i.e. when only a few predictors actually influence the response.\n","\n","In practice, however, one doesn’t know the true values of the parameters. So, the choice between Ridge Regression and LASSO can be based on out-of-sample prediction error. Another option is to combine these two approaches in one — see the next section!\n","\n","LASSO’s loss function looks as follows:"]},{"cell_type":"code","metadata":{"id":"zKAAowH-eAk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":78},"executionInfo":{"status":"ok","timestamp":1592450903094,"user_tz":300,"elapsed":411,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"41b97088-904c-4ac8-ddfd-7293d53d333f"},"source":["Image(url=\"https://miro.medium.com/max/620/1*X1KWVThORJD3H72jctDmhQ.png\")"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/620/1*X1KWVThORJD3H72jctDmhQ.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"x3qErVgSeK3o","colab_type":"text"},"source":["Unlike in Ridge Regression, this minimization problem cannot be solved analytically. Fortunately, there are numerical algorithms able to deal with it."]},{"cell_type":"code","metadata":{"id":"RlCeP1L7eInd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1592450913202,"user_tz":300,"elapsed":511,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"d6c09b57-93dc-4a53-8d68-3ca59c80feea"},"source":["lasso_cv = LassoCV(normalize=True, alphas=np.logspace(-10, 1, 400))\n","lasso_model = lasso_cv.fit(X_train, y_train)\n","lasso_prediction = lasso_model.predict(X_test)\n","lasso_mae = np.mean(np.abs(y_test - lasso_prediction))\n","lasso_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((lasso_model.intercept_, lasso_model.coef_), axis=None), 3))\n",")\n","\n","print('LASSO MAE: {}'.format(np.round(lasso_mae, 3)))\n","print('LASSO coefficients:')\n","lasso_coefs"],"execution_count":19,"outputs":[{"output_type":"stream","text":["LASSO MAE: 0.222\n","LASSO coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 0.597,\n"," 'Unnamed: 0': 0.035,\n"," 'age': -0.0,\n"," 'gleason': 0.0,\n"," 'lbph': 0.021,\n"," 'lcavol': 0.096,\n"," 'lcp': 0.0,\n"," 'lweight': 0.0,\n"," 'pgg45': 0.0,\n"," 'svi': 0.0}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"8Yaa4uvWeTc_","colab_type":"text"},"source":["Elastic Net\n","\n","Elastic Net first emerged as a result of critique on LASSO, whose variable selection can be too dependent on data and thus unstable. Its solution is to combine the penalties of Ridge Regression and LASSO to get the best of both worlds. Elastic Net aims at minimizing the loss function that includes both the L1 and L2 penalties:"]},{"cell_type":"code","metadata":{"id":"CbjbNhJMeWm1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1592450979722,"user_tz":300,"elapsed":351,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"47e78af4-cf47-4efd-927a-c69640d75ec9"},"source":["Image(url=\"https://miro.medium.com/max/942/1*435ISABMIkEjwvkBDrpsQQ.png\")"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/max/942/1*435ISABMIkEjwvkBDrpsQQ.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"mxOHfq5KeWzL","colab_type":"text"},"source":["where α is the mixing paramter between Ridge Regression (when it is zero) and LASSO (when it is one). The best α can be chosen with scikit-learn’s cross-validation-based hyperparaneter tuning."]},{"cell_type":"code","metadata":{"id":"DKihZmkwecKr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1592451009057,"user_tz":300,"elapsed":17039,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"dca37051-b4d1-4bd0-af8c-3c03df55225e"},"source":["elastic_net_cv = ElasticNetCV(normalize=True, alphas=np.logspace(-10, 1, 400), \n","                              l1_ratio=np.linspace(0, 1, 100))\n","elastic_net_model = elastic_net_cv.fit(X_train, y_train)\n","elastic_net_prediction = elastic_net_model.predict(X_test)\n","elastic_net_mae = np.mean(np.abs(y_test - elastic_net_prediction))\n","elastic_net_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((elastic_net_model.intercept_, \n","                                 elastic_net_model.coef_), axis=None), 3))\n",")\n","\n","print('Elastic Net MAE: {}'.format(np.round(elastic_net_mae, 3)))\n","print('Elastic Net coefficients:')\n","elastic_net_coefs"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Elastic Net MAE: 0.222\n","Elastic Net coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 0.597,\n"," 'Unnamed: 0': 0.035,\n"," 'age': -0.0,\n"," 'gleason': 0.0,\n"," 'lbph': 0.021,\n"," 'lcavol': 0.096,\n"," 'lcp': 0.0,\n"," 'lweight': 0.0,\n"," 'pgg45': 0.0,\n"," 'svi': 0.0}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"wHqbCeJYelNU","colab_type":"text"},"source":["# Least Angle Regression\n","\n","So far we have discussed one subsetting method, Best Subset Regression, and three shrinkage methods: Ridge Regression, LASSO and their combination, Elastic Net. This section is devoted to an approach located somewhere in between subsetting and shrinking: Least Angle Regression (LAR). This algorithm starts with a null model, with all coefficients equal to zero, and then works iteratively, at each step moving the coefficient of one of the variables towards its least squares value.\n","\n","More specifically, LAR starts with identifying the variable most correlated with the response. Then it moves the coefficient of this variable continuously toward its leasts squares value, thus decreasing its correlation with the evolving residual. As soon as another variable “catches up” in terms of correlation with the residual, the process is paused. The second variable then joins the active set, i.e. the set of variables with non-zero coefficients, and their coefficients are moved together in a way that keeps their correlations tied and decreasing. This process is continued until all the variables are in the model, and ends at the full least-squares fit. The name “Least Angle Regression” comes from the geometrical interpretation of the algorithm in which the new fit direction at a given step makes the smallest angle with each of the features that already have non-zero coefficents.\n","\n","The code chunk below applies LAR to the prostate data."]},{"cell_type":"code","metadata":{"id":"rCiyuUvVeo0s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1592451045557,"user_tz":300,"elapsed":418,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"a873f030-f9ef-429c-be98-2f79b7a47cc5"},"source":["LAR_cv = LarsCV(normalize=True)\n","LAR_model = LAR_cv.fit(X_train, y_train)\n","LAR_prediction = LAR_model.predict(X_test)\n","LAR_mae = np.mean(np.abs(y_test - LAR_prediction))\n","LAR_coefs = dict(\n","    zip(['Intercept'] + data.columns.tolist()[:-1], \n","        np.round(np.concatenate((LAR_model.intercept_, LAR_model.coef_), axis=None), 3))\n",")\n","\n","print('Least Angle Regression MAE: {}'.format(np.round(LAR_mae, 3)))\n","print('Least Angle Regression coefficients:')\n","LAR_coefs"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Least Angle Regression MAE: 0.222\n","Least Angle Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 0.598,\n"," 'Unnamed: 0': 0.035,\n"," 'age': 0.0,\n"," 'gleason': 0.0,\n"," 'lbph': 0.02,\n"," 'lcavol': 0.096,\n"," 'lcp': 0.0,\n"," 'lweight': 0.0,\n"," 'pgg45': 0.0,\n"," 'svi': 0.0}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"m2LKDAqaf_Cz","colab_type":"text"},"source":["# Principal Components Regression\n","\n","We have already discussed methods for choosing variables (subsetting) and decreasing their coefficients (shrinkage). The last two methods explained in this article take a slightly different approach: they squeeze the input space of the original features into a lower-dimensional space. Mainly, they use X to create a small set of new features Z that are linear combinations of X and then use those in regression models.\n","\n","The first of these two methods is Principal Components Regression. It applies Principal Components Analysis, a method allowing to obtain a set of new features, uncorrelated with each other and having high variance (so that they can explain the variance of the target), and then uses them as features in simple linear regression. This makes it similar to Ridge Regression, as both of them operate on the principal components space of the original features (for PCA-based derivation of Ridge Regression see [1] in Sources at the bottom of this article). The difference is that PCR discards the components with least informative power, while Ridge Regression simply shrinks them stronger.\n","\n","The number of components to reatain can be viewed as a hyperparameter and tuned via cross-validation, as is the case in the code chunk below."]},{"cell_type":"code","metadata":{"id":"9PjkNtPxf_PY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592451400097,"user_tz":300,"elapsed":582,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"4fed0a08-0f2c-41f6-90ad-e99ef65d6c11"},"source":["regression_model = LinearRegression(normalize=True)\n","pca_model = PCA()\n","pipe = Pipeline(steps=[('pca', pca_model), ('least_squares', regression_model)])\n","param_grid = {'pca__n_components': range(1, 9)}\n","search = GridSearchCV(pipe, param_grid)\n","pcareg_model = search.fit(X_train, y_train)\n","pcareg_prediction = pcareg_model.predict(X_test)\n","pcareg_mae = np.mean(np.abs(y_test - pcareg_prediction))\n","n_comp = list(pcareg_model.best_params_.values())[0]\n","pcareg_coefs = dict(\n","   zip(['Intercept'] + ['PCA_comp_' + str(x) for x in range(1, n_comp + 1)], \n","       np.round(np.concatenate((pcareg_model.best_estimator_.steps[1][1].intercept_, \n","                                pcareg_model.best_estimator_.steps[1][1].coef_), axis=None), 3))\n",")\n","\n","print('Principal Components Regression MAE: {}'.format(np.round(pcareg_mae, 3)))\n","print('Principal Components Regression coefficients:')\n","pcareg_coefs"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Principal Components Regression MAE: 0.245\n","Principal Components Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Intercept': 2.452, 'PCA_comp_1': 0.029, 'PCA_comp_2': -0.026}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"exZJlb_sjS9s","colab_type":"text"},"source":["# Partial Least Squares\n","\n","The final method discussed in this article is Partial Least Squares (PLS). Similarly to Principal Components Regression, it also uses a small set of linear combinations of the original features. The difference lies in how these combinations are constructed. While Principal Components Regression uses only X themselves to create the derived features Z, Partial Least Squares additionally uses the target y. Hence, while constructing Z, PLS seeks directions that have high variance (as these can explain variance in the target) and high correlation with the target. This stays in contrast to the principal components appraoch, which focuses on high variance only.\n","\n","Under the hood of the algorithm, the first of the new features, z1, is created as a linear combination of all features X, where each of the Xs is weighted by its inner product with the target y. Then, y is regressed on z1 giving PLS β-coefficients. Finally, all X are orthogonalized with respect to z1. Then the process starts anew for z2 and goes on until the desired numbers of components in Z is obtained. This number, as usual, can be chosen via cross-validation.\n","\n","It can be shown that although PLS shrinks the low-variance components in Z as desired, it can sometimes inflate the high-variance ones, which might lead to higher prediction errors in some cases. This seems to be the case for our prostate data: PLS performs the worst among all discussed methods."]},{"cell_type":"code","metadata":{"id":"jUEBuB8hjW1a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1592452283403,"user_tz":300,"elapsed":567,"user":{"displayName":"Ali Nemati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPRR6BQuQhnxuekZV5YGNDjSSnI5Q9QRiz4qchR2M=s64","userId":"16088017015746410747"}},"outputId":"c7daf2d4-5f15-4342-c5dc-8a23cb224017"},"source":["pls_model_setup = PLSRegression(scale=True)\n","param_grid = {'n_components': range(1, 9)}\n","search = GridSearchCV(pls_model_setup, param_grid)\n","pls_model = search.fit(X_train, y_train)\n","pls_prediction = pls_model.predict(X_test)\n","pls_mae = np.mean(np.abs(y_test - pls_prediction))\n","pls_coefs = dict(\n","  zip(data.columns.tolist()[:-1], \n","      np.round(np.concatenate((pls_model.best_estimator_.coef_), axis=None), 3))\n",")\n","\n","print('Partial Least Squares Regression MAE: {}'.format(np.round(pls_mae, 3)))\n","print('Partial Least Squares Regression coefficients:')\n","pls_coefs"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Partial Least Squares Regression MAE: 1.083\n","Partial Least Squares Regression coefficients:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Unnamed: 0': 0.988,\n"," 'age': -0.045,\n"," 'gleason': -0.028,\n"," 'lbph': 0.056,\n"," 'lcavol': 0.214,\n"," 'lcp': -0.116,\n"," 'lweight': 0.033,\n"," 'pgg45': 0.094,\n"," 'svi': 0.049}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"TG3-xCkUjZt-","colab_type":"text"},"source":["# Recap & Conclusions\n","\n","With many, possibly correlated features, linear models fail in terms of prediction accuracy and model’s interpretability due to large variance of model’s parameters. This can be alleviated by reducing the variance, which can only happen at the cost of introducing some bias. Yet, finding the best bias-variance trade-off can optimize model’s performance.\n","\n","Two broad classes of approaches allowing to achieve this are subsetting and shrinkage. The former selects a subset of variables, while the latter shrinks the coefficients of the model towards zero. Both approaches result in a reduction of model’s complexity, which leads to the desired decrease in parameters’ variance.\n","\n","This article discussed a couple of subsetting and shrinkage methods:\n","\n","+ **Best Subset Regression** iterates over all possible feature combination to select the best one;\n","+ **Ridge Regression penalizes** the squared coefficient values (L2 penalty) enforcing them to be small;\n","+ **LASSO penalizes** the absolute values of the coefficients (L1 penalty) which can force some of them to be exactly zero;\n","+ **Elastic Net** combines the L1 and L2 penalties, enjoying the best of Ridge and Lasso;\n","+ **Least Angle Regression** fits in between subsetting and shrinkage: it works iteratively, adding “some part” of one of the features at each step;\n","+ **Principal Components Regression** performs PCA to squeeze the original features into a small subset of new features and then uses those as predictors;\n","+ **Partial Least Squares** also summarizes original features into a smaller subset of new ones, but unlike PCR, it also makes use of the targets to construct them.\n","\n","As you will see from the applications to the prostate data if you run the code chunks above, most of these methods perform similarly in terms of prediction accuracy. The first 5 methods’ errors range between 0.467 and 0.517, beating least squares’ error of 0.523. The last two, PCR and PLS, perform worse, possibily due to the fact that there are not that many features in the data, hence gains from dimensionality reduction are limited."]}]}